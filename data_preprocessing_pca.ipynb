{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "X = pd.read_csv('x_cleaned_pca.csv')\n",
    "y = pd.read_csv('y_cleaned_pca.csv')\n",
    "\n",
    "quality_and_conditions=[]\n",
    "\n",
    "for columns in X.columns:\n",
    "    if '_Qual' in columns or '_Cond' in columns:\n",
    "        quality_and_conditions.append(columns)\n",
    "\n",
    "quality_and_conditions.remove('Sale_Condition')\n",
    "quality_and_conditions.remove('Low_Qual_Fin_SF')\n",
    "quality_and_conditions.append('Bsmt_Exposure')\n",
    "quality_and_conditions.append('Heating_QC')\n",
    "quality_and_conditions.append('Fireplace_Qu')\n",
    "\n",
    "X.loc[:,'All_Quality'] = X.loc[:, quality_and_conditions].sum(axis=1)\n",
    "\n",
    "# perform groupby on `Neighborhood`, then aggregate via averaging\n",
    "neighborhood_scores = X.loc[:, ['Neighborhood','All_Quality']].groupby('Neighborhood').agg(Mean_Agg_Score=('All_Quality', np.mean))\n",
    "\n",
    "# sort dataframe by descending order of the mean score\n",
    "neighborhood_scores.sort_values(by='Mean_Agg_Score', ascending=False, inplace=True)\n",
    "\n",
    "# extract out neighborhood names from the index\n",
    "neighborhood_scores.loc[:,'Neighborhood'] = neighborhood_scores.index\n",
    "\n",
    "# reset index to the usual numeric form\n",
    "neighborhood_scores.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# re-arrange columns to being Neighborhood to left-most position\n",
    "neighborhood_scores = neighborhood_scores.loc[:, ['Neighborhood', 'Mean_Agg_Score']]\n",
    "\n",
    "\n",
    "def get_score(x):\n",
    "    return neighborhood_scores.loc[neighborhood_scores.loc[:,'Neighborhood']==x,'Mean_Agg_Score'].values[0]\n",
    "\n",
    "X.loc[:,'Neighborhood_Score'] = X.loc[:,'Neighborhood'].apply(get_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creiamo una nuova feature Total_Qual che Ã¨ pari alla somma di tutti i campi qualitativi\n",
    "Creiamo una nuova feature Neighborhood_Score che si basa su Total_Qual per assegnargli un valore qualitativo al posto di nominale\n",
    "\n",
    "TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "0       272.0\n1       260.0\n2       429.0\n3         0.0\n4       246.0\n        ...  \n2919    120.0\n2920    164.0\n2921    112.0\n2922    278.0\n2923    238.0\nName: Total_External_SF, Length: 2924, dtype: float64"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "porch = ['Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch']\n",
    "X.loc[:,'Total_External_SF'] = X.loc[:, porch].sum(axis=1)\n",
    "\n",
    "display(X['Total_External_SF'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Total_External_SF\n",
    "Creating a new feature that represents the surface of all porch or decks outside the property, trying to combine similar information into a single feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "0       2823.0\n1       2238.0\n2       2564.0\n3       3697.0\n4       2902.0\n         ...  \n2919    2410.0\n2920    2011.0\n2921    1307.0\n2922    3001.0\n2923    3408.0\nName: Total_SF, Length: 2924, dtype: float64"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "surface = ['Total_Finished_Bsmt_SF', 'First_Flr_SF', 'Second_Flr_SF', 'Garage_Area']\n",
    "X['Total_Finished_Bsmt_SF'] = X['Total_Bsmt_SF'] - X['Bsmt_Unf_SF']\n",
    "X.loc[:, 'Total_SF'] = X.loc[:, surface].sum(axis=1)\n",
    "#vediamo se fare anche Total senza unfinished\n",
    "\n",
    "display(X['Total_SF'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Total_SF\n",
    "Creating a new feature that represents the surface of the inside of the property, trying to combine similar information into a single feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "0       2.0\n1       1.0\n2       2.0\n3       4.0\n4       3.0\n       ... \n2919    2.0\n2920    2.0\n2921    2.0\n2922    2.0\n2923    3.0\nName: Total_Baths, Length: 2924, dtype: float64"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "baths = ['Full_Bath', 'Half_Bath', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath']\n",
    "X.loc[:, 'Total_Baths'] = X.loc[:, baths].sum(axis=1)\n",
    "\n",
    "display(X['Total_Baths'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Total_Baths\n",
    "Creating a new feature that represents the sum of all the types of bathrooms, trying to add new information to the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "0       50.0\n1       49.0\n2       52.0\n3       42.0\n4       13.0\n        ... \n2919    22.0\n2920    23.0\n2921    14.0\n2922    32.0\n2923    13.0\nName: Year_To_Sell, Length: 2924, dtype: float64"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X.loc[:,'Year_To_Sell'] = X.loc[:,'Year_Sold'] - X.loc[:,'Year_Built']\n",
    "\n",
    "display(X['Year_To_Sell'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Year_To_Sell\n",
    "Creating a new feature that represents the span of years required to sell the house, trying to add new information to the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: \"b'One_Story_1946_and_Newer_All_Styles'\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecomposition\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PCA\n\u001B[0;32m      3\u001B[0m pca_train \u001B[38;5;241m=\u001B[39m PCA(n_components\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m215\u001B[39m,random_state\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m \u001B[43mpca_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m pca_train\u001B[38;5;241m.\u001B[39mcomponents_\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# project data onto the selected components\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:382\u001B[0m, in \u001B[0;36mPCA.fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    366\u001B[0m     \u001B[38;5;124;03m\"\"\"Fit the model with X.\u001B[39;00m\n\u001B[0;32m    367\u001B[0m \n\u001B[0;32m    368\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    380\u001B[0m \u001B[38;5;124;03m        Returns the instance itself.\u001B[39;00m\n\u001B[0;32m    381\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 382\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    383\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:430\u001B[0m, in \u001B[0;36mPCA._fit\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    424\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m issparse(X):\n\u001B[0;32m    425\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    426\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPCA does not support sparse input. See \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    427\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTruncatedSVD for a possible alternative.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    428\u001B[0m     )\n\u001B[1;32m--> 430\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    431\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat64\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mensure_2d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\n\u001B[0;32m    432\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    434\u001B[0m \u001B[38;5;66;03m# Handle n_components==None\u001B[39;00m\n\u001B[0;32m    435\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_components \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:566\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    564\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation should be done on X, y or both.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    565\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[1;32m--> 566\u001B[0m     X \u001B[38;5;241m=\u001B[39m check_array(X, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    567\u001B[0m     out \u001B[38;5;241m=\u001B[39m X\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001B[0m\n\u001B[0;32m    744\u001B[0m         array \u001B[38;5;241m=\u001B[39m array\u001B[38;5;241m.\u001B[39mastype(dtype, casting\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munsafe\u001B[39m\u001B[38;5;124m\"\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    745\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 746\u001B[0m         array \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ComplexWarning \u001B[38;5;28;01mas\u001B[39;00m complex_warning:\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    749\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mComplex data not supported\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(array)\n\u001B[0;32m    750\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcomplex_warning\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2064\u001B[0m, in \u001B[0;36mNDFrame.__array__\u001B[1;34m(self, dtype)\u001B[0m\n\u001B[0;32m   2063\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__array__\u001B[39m(\u001B[38;5;28mself\u001B[39m, dtype: npt\u001B[38;5;241m.\u001B[39mDTypeLike \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[1;32m-> 2064\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: could not convert string to float: \"b'One_Story_1946_and_Newer_All_Styles'\""
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_train = PCA(n_components=215,random_state= 1)\n",
    "pca_train.fit(X)\n",
    "pca_train.components_\n",
    "\n",
    "# project data onto the selected components\n",
    "X_proj_train = pca_train.transform(X)\n",
    "#df_train = pd.DataFrame(data=X_proj_train, columns=cols)\n",
    "df_train = pd.DataFrame(X_proj_train)\n",
    "display(df_train)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PCA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4, random_state=1, shuffle=True)\n",
    "\n",
    "for train, test in skf.split(X, y):\n",
    "    X_train = X.iloc[train]\n",
    "    y_train = y.iloc[train]\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SPLIT AFTER PCA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X_train.loc[:,[col for col in X_train.columns]])\n",
    "print('x_train_v2 shape after get_dummies: {}'.format(X_train.shape))\n",
    "X_test = pd.get_dummies(X_test.loc[:,[col for col in X_test.columns]])\n",
    "print('x_test_v2 shape after get_dummies: {}'.format(X_test.shape))\n",
    "\n",
    "for col in X_train.columns:\n",
    "    if col not in X_test.columns:\n",
    "        print('\\'{}\\' found missing in x_test_v2, initialising new column with 0s.'.format(col))\n",
    "        X_test.loc[:,col] = 0\n",
    "print('\\n')\n",
    "for col in X_test.columns:\n",
    "    if col not in X_train.columns:\n",
    "        print('\\'{}\\' found missing in x_train_v2, initialising new column with 0s.'.format(col))\n",
    "        X_train.loc[:,col] = 0\n",
    "print('\\n')\n",
    "print('Check if both datasets have the same set of columns: {}'.format(set(X_train.columns) == set(X_test.columns)))\n",
    "print('\\n')\n",
    "print('Current number of columns: {}'.format(X_train.shape[1]))\n",
    "\n",
    "# ensure that column sequence is similar too\n",
    "column_list = list(X_train.columns)\n",
    "X_test = X_test.loc[:,column_list]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# One Hot Encoding\n",
    "\n",
    "Applying the one hot encoding using the get_dummies function of pandas.\n",
    "Since it's possible that a nominal value is not present in one of the categorical columns, resulting in more or lesser columns generated in train split vs test split.\n",
    "Assuring to reach the same of features by filling with 0 the ones that are missing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "cols = X_train.columns\n",
    "\n",
    "# perform the normalization\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.fit_transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train, columns=cols)\n",
    "X_test = pd.DataFrame(data=X_test, columns=cols)\n",
    "\n",
    "X_train.to_csv('x_train_preprocessed_pca.csv', index=False)\n",
    "y_train.to_csv('y_train_preprocessed_pca.csv', index=False)\n",
    "X_test.to_csv('x_test_preprocessed_pca.csv', index=False)\n",
    "y_test.to_csv('y_test_preprocessed_pca.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Standard Scaler\n",
    "Standardizing features by removing the mean and scaling to unit variance (-1, 1). Work best with normal variance features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "cols = X_train.columns\n",
    "\n",
    "# perform the normalization\n",
    "X_train = mm.fit_transform(X_train)\n",
    "X_test = mm.fit_transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train, columns=cols)\n",
    "X_test = pd.DataFrame(data=X_test, columns=cols)\n",
    "\n",
    "X_train.to_csv('x_train_preprocessed_minmax_pca.csv', index=False)\n",
    "y_train.to_csv('y_train_preprocessed_minmax_pca.csv', index=False)\n",
    "X_test.to_csv('x_test_preprocessed_minmax_pca.csv', index=False)\n",
    "y_test.to_csv('y_test_preprocessed_minmax_pca.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Min Max Scaler\n",
    "Transforming features by scaling each feature to a given range (0, 1)."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
