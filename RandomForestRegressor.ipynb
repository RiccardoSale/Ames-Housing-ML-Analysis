{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Standard\\AppData\\Local\\Temp/ipykernel_3128/1454253534.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf_model.fit(X_train,y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST:  0.8675848465690074\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train = pd.read_csv('x_train_preprocessed.csv')\n",
    "X_test = pd.read_csv('x_test_preprocessed.csv')\n",
    "y_train = pd.read_csv('y_train_preprocessed.csv')\n",
    "y_test = pd.read_csv('y_test_preprocessed.csv')\n",
    "\n",
    "rf_model = RandomForestRegressor(max_leaf_nodes=43, max_depth=9, min_samples_leaf=11, criterion='mae', max_features='auto', bootstrap=True, n_jobs=-1 )\n",
    "rf_model.fit(X_train,y_train)\n",
    "print(\"TEST: \",rf_model.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Svantaggio del Random Forest è la computational complexity dato che tutti gli alberi nella foresta devono fare una predizione, tra tutti i voti ne scegliamo uno.\n",
    "Rispetto al decision Tree è molto più difficile da interpretare a livello visivo.\n",
    "Vantaggi: Soffre meno di overfitting in quanto fa l'average tra tutte le predizioni, cancellando il bias. Può essere usato come algoritmo per il feature selection\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Standard\\AppData\\Local\\Temp/ipykernel_3128/1772905564.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf_model.fit(X_train,y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  0.9258816816599326\n",
      "TEST:  0.8797011534919679\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor(max_leaf_nodes=70, max_depth=10, min_samples_leaf=5, criterion='mae', max_features='auto', bootstrap=True, n_jobs=-1 )\n",
    "rf_model.fit(X_train,y_train)\n",
    "print(\"TRAIN: \",rf_model.score(X_train,y_train))\n",
    "print(\"TEST: \",rf_model.score(X_test,y_test))\n",
    "\n",
    "#f_i = list(zip(X_train.columns,rf_model.feature_importances_))\n",
    "#f_i.sort(key = lambda x : x[1])\n",
    "#plt.barh([x[0] for x in f_i],[x[1] for x in f_i])\n",
    "\n",
    "#plt.rcParams[\"figure.figsize\"]=250,250\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dopo diversi test possiamo notare come si può rendere i vari alberi più precisi dato che la natura del random forest regressor va a ridurre l'overfitting attraverso l'average dei vari alberi, notiamo infatti come si riesca ad ottenere una precisione più elevata con un basso overfitting alzando leggermente il numero di max depth e max leaf nodes e riducendo i sample necessari per creare una nuova foglia"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Standard\\AppData\\Local\\Temp/ipykernel_3128/2723546116.py:23: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf_model.fit(X_train,y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  0.9218363330412896\n",
      "TEST:  0.8831846493144154\n"
     ]
    }
   ],
   "source": [
    "oh_neighbor = []\n",
    "for col in X_train.columns:\n",
    "    if 'Neighborhood_b' in col:\n",
    "        oh_neighbor.append(col)\n",
    "\n",
    "X_train.drop(columns=oh_neighbor, inplace=True)\n",
    "X_test.drop(columns=oh_neighbor, inplace=True)\n",
    "\n",
    "porch = ['Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch']\n",
    "surface = ['Total_Finished_Bsmt_SF', 'First_Flr_SF', 'Second_Flr_SF', 'Garage_Area']\n",
    "baths = ['Full_Bath', 'Half_Bath', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath']\n",
    "\n",
    "X_train.drop(columns=porch, inplace=True)\n",
    "X_test.drop(columns=porch, inplace=True)\n",
    "\n",
    "X_train.drop(columns=surface, inplace=True)\n",
    "X_test.drop(columns=surface, inplace=True)\n",
    "\n",
    "X_train.drop(columns=baths, inplace=True)\n",
    "X_test.drop(columns=baths, inplace=True)\n",
    "\n",
    "rf_model = RandomForestRegressor(max_leaf_nodes=70, max_depth=10, min_samples_leaf=5, criterion='mae', max_features='auto', bootstrap=True, n_jobs=-1 )\n",
    "rf_model.fit(X_train,y_train)\n",
    "print(\"TRAIN: \",rf_model.score(X_train,y_train))\n",
    "print(\"TEST: \",rf_model.score(X_test,y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notiamo che le variabili eliminate non erano significative per l'apprendimento del modello, anzi notiamo un lieve miglioramento sul lato overfitting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Standard\\AppData\\Local\\Temp/ipykernel_3128/265048710.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf_model.fit(X_train,y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  0.8845609162090243\n",
      "TEST:  0.8365168661545757\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor(max_leaf_nodes=70, max_depth=10, min_samples_leaf=5, criterion='mae', max_features='sqrt', bootstrap=True, n_jobs=-1, n_estimators=100 )\n",
    "rf_model.fit(X_train,y_train)\n",
    "print(\"TRAIN: \",rf_model.score(X_train,y_train))\n",
    "print(\"TEST: \",rf_model.score(X_test,y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notiamo che utilizzando solamente le migliori sqrt (di n features) andiamo a peggiorare la stima del modello, da questa osservazione si può intuire che tutte le features presenti nel dataset siano importanti"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
