{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor - Ensemblee\n",
    "Random forest is a supervised machine learning algorithm that can be used for both classification and regression tasks. It works by constructing a large number of decision trees during the training phase and then combining the predictions made by each individual tree to produce a final prediction.\n",
    "\n",
    "In a random forest regressor, each tree in the forest makes a prediction based on the features in the training data and the final prediction is the average of the predictions made by all the trees in the forest. This process helps to reduce the variance in the predictions made by the model, which can improve the overall accuracy of the model.\n",
    "\n",
    "One key feature of the random forest algorithm is that it randomly selects a subset of features to use in each decision tree. This helps to prevent overfitting, which occurs when a model becomes too closely fitted to the training data and performs poorly on new, unseen data. By using a random subset of features in each tree, the random forest model is able to generalize better to new data.\n",
    "\n",
    "\n",
    "- Disadvantage of Random Forest is its computational complexity since all the trees in the forest have to make a prediction, among all the votes we choose one.\n",
    "- Compared to Decision Tree it is much more difficult to visually interpret.\n",
    "- Advantages: It suffers less from overfitting since it does the average among all predictions, cancelling out the bias. Can be used as an algorithm for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Testing\n",
    "Trying the algorithm with parameters similar to those obtained from the decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST:  0.9064452492922744\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#muting warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_train = pd.read_csv('x_train_preprocessed.csv')\n",
    "X_test = pd.read_csv('x_test_preprocessed.csv')\n",
    "y_train = pd.read_csv('y_train_preprocessed.csv')\n",
    "y_test = pd.read_csv('y_test_preprocessed.csv')\n",
    "\n",
    "rf_model = RandomForestRegressor(max_leaf_nodes=43, max_depth=9, min_samples_leaf=11, criterion='mae', max_features='auto', bootstrap=True, n_jobs=-1 )\n",
    "rf_model.fit(X_train,y_train)\n",
    "print(\"TEST: \",rf_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and reasoning\n",
    "Based on our knowledge of the algorithm and the information observed from the decisionTree analysis, until we obtain observations that satisfy us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  0.9241219721496292\n",
      "TEST:  0.9168053961454663\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "rf_model = RandomForestRegressor(max_leaf_nodes=70, max_depth=10, min_samples_leaf=5, criterion='mae', max_features='auto', bootstrap=True, n_jobs=-1 )\n",
    "rf_model.fit(X_train,y_train)\n",
    "print(\"TRAIN: \",rf_model.score(X_train,y_train))\n",
    "print(\"TEST: \",rf_model.score(X_test,y_test))\n",
    "\n",
    "#Create image based on feature_importances\n",
    "f_i = list(zip(X_train.columns,rf_model.feature_importances_))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"]=250,250\n",
    "plt.show()\n",
    "#images need to be saved too big to visualise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning after testing multiple parameters\n",
    "- After several tests we can see how the various trees can be made more accurate since the nature of the random forest regressor goes to reduce overfitting through the average of the various trees.\n",
    "- In fact, we notice how higher accuracy can be achieved with lower overfitting by slightly raising the number of max depth and max leaf nodes, reducing the samples needed to create a new leaf.\n",
    "- Thus taking advantages of the nature of RandomForest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing components of custom feature ( Similar to Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  0.9231321687716705\n",
      "TEST:  0.9209663430611782\n"
     ]
    }
   ],
   "source": [
    "oh_neighbor = []\n",
    "for col in X_train.columns:\n",
    "    if 'Neighborhood_b' in col:\n",
    "        oh_neighbor.append(col)\n",
    "\n",
    "X_train.drop(columns=oh_neighbor, inplace=True)\n",
    "X_test.drop(columns=oh_neighbor, inplace=True)\n",
    "\n",
    "porch = ['Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch']\n",
    "surface = ['Total_Finished_Bsmt_SF', 'First_Flr_SF', 'Second_Flr_SF', 'Garage_Area']\n",
    "baths = ['Full_Bath', 'Half_Bath', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath']\n",
    "\n",
    "X_train.drop(columns=porch, inplace=True)\n",
    "X_test.drop(columns=porch, inplace=True)\n",
    "\n",
    "X_train.drop(columns=surface, inplace=True)\n",
    "X_test.drop(columns=surface, inplace=True)\n",
    "\n",
    "X_train.drop(columns=baths, inplace=True)\n",
    "X_test.drop(columns=baths, inplace=True)\n",
    "\n",
    "rf_model = RandomForestRegressor(max_leaf_nodes=70, max_depth=10, min_samples_leaf=5, criterion='mae', max_features='auto', bootstrap=True, n_jobs=-1 )\n",
    "rf_model.fit(X_train,y_train)\n",
    "print(\"TRAIN: \",rf_model.score(X_train,y_train))\n",
    "print(\"TEST: \",rf_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observe that the removed variables were not significant for model learning; in fact, we note a slight improvement on the overfitting side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  0.8795231242150737\n",
      "TEST:  0.8897018386485049\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor(max_leaf_nodes=70, max_depth=10, min_samples_leaf=5, criterion='mae', max_features='sqrt', bootstrap=True, n_jobs=-1, n_estimators=100 )\n",
    "rf_model.fit(X_train,y_train)\n",
    "print(\"TRAIN: \",rf_model.score(X_train,y_train))\n",
    "print(\"TEST: \",rf_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing number of features\n",
    "Note how by using only the best sqrt (of n features) the performance of the estimation of the model is worse, from this observation we can guess that almost all the features in the dataset add information to the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
