{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## ANN Regressor\n",
    "\n",
    "An ANN regressor is a type of artificial neural network (ANN) that is used for regression tasks.\n",
    "\n",
    "ANN is composed of layers of interconnected \"neurons,\" which process and transmit information through the network. Each neuron receives input from the previous layer, processes it using an activation function, and then passes it on to the next layer. The output layer of the ANN regressor produces a prediction for the desired output value, in this case the prediction is on the Sale price of Ames housing database.\n",
    "\n",
    "The weights and biases of the neurons are adjusted during training, using an optimization algorithm such as Adam.\n",
    "The goal of training is to find a set of weights and biases that minimizes the error between the predicted output and the true output values in the training data.\n",
    "\n",
    "Once trained, the ANN regressor can be used to make predictions on new input data by passing it through the network and using the trained weights and biases to calculate the predicted output value.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "import pandas as pd\n",
    "from tensorflow_addons.metrics import RSquare\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train = pd.read_csv('x_train_preprocessed_minmax.csv')\n",
    "X_test = pd.read_csv('x_test_preprocessed_minmax.csv')\n",
    "y_train = pd.read_csv('y_train_preprocessed_minmax.csv')\n",
    "y_test = pd.read_csv('y_test_preprocessed_minmax.csv')\n",
    "\n",
    "y_train = y_train.to_numpy().flatten() #y_train flattened, works better\n",
    "\n",
    "oh_neighbor = []\n",
    "for col in X_train.columns:\n",
    "    if 'Neighborhood_b' in col:\n",
    "        oh_neighbor.append(col)\n",
    "\n",
    "X_train.drop(columns=oh_neighbor, inplace=True)\n",
    "X_test.drop(columns=oh_neighbor, inplace=True)\n",
    "\n",
    "porch = ['Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch']\n",
    "surface = ['Total_Finished_Bsmt_SF', 'First_Flr_SF', 'Second_Flr_SF', 'Garage_Area']\n",
    "baths = ['Full_Bath', 'Half_Bath', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath']\n",
    "\n",
    "X_train.drop(columns=porch, inplace=True)\n",
    "X_test.drop(columns=porch, inplace=True)\n",
    "\n",
    "X_train.drop(columns=surface, inplace=True)\n",
    "X_test.drop(columns=surface, inplace=True)\n",
    "\n",
    "X_train.drop(columns=baths, inplace=True)\n",
    "X_test.drop(columns=baths, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the feature which have been encoded are removed, this solution significatively reduces the loss parameter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Network initialization and setup\n",
    "\n",
    "Using single hidden layer with half of the units of the input one.\n",
    "The output layer has a single units due to the necessity to predict only the Sale Price.\n",
    "\n",
    "For the input and hidden layers the activation function chosen is relu because of its good speed of train and simplicity to compute.\n",
    "The output has instead the linear activation function for predicting the float value.\n",
    "\n",
    "The kernel initializer is always normal, so it use a normal distribution to generate tensors.\n",
    "- Parameters of layers\n",
    "    1) units = number of neurons\n",
    "    2) kernel_initializer = define the distribution used to inizialize the layer\n",
    "    3) activation = activation is a function that is applied to the output do decide the use or not of the neuron\n",
    "    4) input_shape = reflect the number of feature in the dataset\n",
    "- Parameters of optimizer\n",
    "    1) learning_rate = step size at which the algorithm makes update to the model / how fast the model learn , slower learner requires more epoch\n",
    "    2) beta_1 = momentum term in Adam algorithm, larger value more enphasis on the past\n",
    "    3) beta_2 = decay rate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(units=189, kernel_initializer='normal', activation='relu', input_shape=[188]))\n",
    "model.add(layers.Dense(units=95, kernel_initializer='normal', activation='relu'))\n",
    "model.add(layers.Dense(units=1, kernel_initializer='normal', activation=None)) #last layer should be linear\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=0.0015, beta_1=0.9, beta_2=0.999)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/140\n",
      "69/69 [==============================] - 1s 3ms/step - loss: 79.3767 - r_square: -5.0960 - val_loss: 46.9596 - val_r_square: -5.1907\n",
      "Epoch 2/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 37.6621 - r_square: -5.0764 - val_loss: 30.3983 - val_r_square: -5.1610\n",
      "Epoch 3/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 26.4114 - r_square: -5.0404 - val_loss: 22.7713 - val_r_square: -5.1167\n",
      "Epoch 4/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 20.4468 - r_square: -4.9919 - val_loss: 18.1408 - val_r_square: -5.0602\n",
      "Epoch 5/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 16.5698 - r_square: -4.9319 - val_loss: 14.8953 - val_r_square: -4.9913\n",
      "Epoch 6/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 13.7267 - r_square: -4.8587 - val_loss: 12.4499 - val_r_square: -4.9095\n",
      "Epoch 7/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 11.5764 - r_square: -4.7755 - val_loss: 10.5850 - val_r_square: -4.8183\n",
      "Epoch 8/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 9.9164 - r_square: -4.6843 - val_loss: 9.1218 - val_r_square: -4.7193\n",
      "Epoch 9/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 8.5602 - r_square: -4.5819 - val_loss: 7.8351 - val_r_square: -4.6026\n",
      "Epoch 10/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 7.3301 - r_square: -4.4589 - val_loss: 6.7084 - val_r_square: -4.4671\n",
      "Epoch 11/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 6.3043 - r_square: -4.3241 - val_loss: 5.7881 - val_r_square: -4.3235\n",
      "Epoch 12/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 5.4579 - r_square: -4.1814 - val_loss: 5.0205 - val_r_square: -4.1721\n",
      "Epoch 13/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 4.7477 - r_square: -4.0329 - val_loss: 4.3707 - val_r_square: -4.0134\n",
      "Epoch 14/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 4.1464 - r_square: -3.8778 - val_loss: 3.8218 - val_r_square: -3.8504\n",
      "Epoch 15/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 3.6328 - r_square: -3.7171 - val_loss: 3.3469 - val_r_square: -3.6814\n",
      "Epoch 16/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 3.1851 - r_square: -3.5513 - val_loss: 2.9322 - val_r_square: -3.5067\n",
      "Epoch 17/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 2.7929 - r_square: -3.3793 - val_loss: 2.5682 - val_r_square: -3.3263\n",
      "Epoch 18/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 2.4518 - r_square: -3.2049 - val_loss: 2.2541 - val_r_square: -3.1453\n",
      "Epoch 19/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 2.1569 - r_square: -3.0303 - val_loss: 1.9829 - val_r_square: -2.9651\n",
      "Epoch 20/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 1.9019 - r_square: -2.8575 - val_loss: 1.7488 - val_r_square: -2.7878\n",
      "Epoch 21/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 1.6814 - r_square: -2.6874 - val_loss: 1.5449 - val_r_square: -2.6128\n",
      "Epoch 22/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 1.4896 - r_square: -2.5201 - val_loss: 1.3683 - val_r_square: -2.4427\n",
      "Epoch 23/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 1.3226 - r_square: -2.3577 - val_loss: 1.2138 - val_r_square: -2.2769\n",
      "Epoch 24/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 1.1763 - r_square: -2.2001 - val_loss: 1.0784 - val_r_square: -2.1158\n",
      "Epoch 25/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 1.0478 - r_square: -2.0458 - val_loss: 0.9599 - val_r_square: -1.9604\n",
      "Epoch 26/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.9350 - r_square: -1.8980 - val_loss: 0.8552 - val_r_square: -1.8097\n",
      "Epoch 27/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.8355 - r_square: -1.7549 - val_loss: 0.7634 - val_r_square: -1.6655\n",
      "Epoch 28/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.7478 - r_square: -1.6174 - val_loss: 0.6821 - val_r_square: -1.5266\n",
      "Epoch 29/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.6703 - r_square: -1.4865 - val_loss: 0.6103 - val_r_square: -1.3937\n",
      "Epoch 30/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.6018 - r_square: -1.3605 - val_loss: 0.5471 - val_r_square: -1.2673\n",
      "Epoch 31/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.5412 - r_square: -1.2405 - val_loss: 0.4911 - val_r_square: -1.1467\n",
      "Epoch 32/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.4874 - r_square: -1.1259 - val_loss: 0.4413 - val_r_square: -1.0316\n",
      "Epoch 33/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.4395 - r_square: -1.0166 - val_loss: 0.3966 - val_r_square: -0.9207\n",
      "Epoch 34/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3965 - r_square: -0.9117 - val_loss: 0.3567 - val_r_square: -0.8147\n",
      "Epoch 35/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3570 - r_square: -0.8086 - val_loss: 0.3189 - val_r_square: -0.7071\n",
      "Epoch 36/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3200 - r_square: -0.7052 - val_loss: 0.2845 - val_r_square: -0.6015\n",
      "Epoch 37/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2863 - r_square: -0.6032 - val_loss: 0.2535 - val_r_square: -0.4994\n",
      "Epoch 38/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2565 - r_square: -0.5064 - val_loss: 0.2263 - val_r_square: -0.4027\n",
      "Epoch 39/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2306 - r_square: -0.4163 - val_loss: 0.2031 - val_r_square: -0.3136\n",
      "Epoch 40/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2085 - r_square: -0.3328 - val_loss: 0.1837 - val_r_square: -0.2334\n",
      "Epoch 41/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1899 - r_square: -0.2581 - val_loss: 0.1672 - val_r_square: -0.1601\n",
      "Epoch 42/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1742 - r_square: -0.1901 - val_loss: 0.1534 - val_r_square: -0.0944\n",
      "Epoch 43/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1610 - r_square: -0.1289 - val_loss: 0.1422 - val_r_square: -0.0366\n",
      "Epoch 44/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1501 - r_square: -0.0748 - val_loss: 0.1324 - val_r_square: 0.0171\n",
      "Epoch 45/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1410 - r_square: -0.0266 - val_loss: 0.1248 - val_r_square: 0.0625\n",
      "Epoch 46/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1335 - r_square: 0.0164 - val_loss: 0.1183 - val_r_square: 0.1040\n",
      "Epoch 47/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1272 - r_square: 0.0546 - val_loss: 0.1130 - val_r_square: 0.1397\n",
      "Epoch 48/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1220 - r_square: 0.0884 - val_loss: 0.1086 - val_r_square: 0.1725\n",
      "Epoch 49/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1177 - r_square: 0.1177 - val_loss: 0.1050 - val_r_square: 0.2004\n",
      "Epoch 50/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1141 - r_square: 0.1439 - val_loss: 0.1020 - val_r_square: 0.2245\n",
      "Epoch 51/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1110 - r_square: 0.1669 - val_loss: 0.0995 - val_r_square: 0.2468\n",
      "Epoch 52/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1084 - r_square: 0.1881 - val_loss: 0.0973 - val_r_square: 0.2665\n",
      "Epoch 53/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1062 - r_square: 0.2064 - val_loss: 0.0954 - val_r_square: 0.2844\n",
      "Epoch 54/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1042 - r_square: 0.2231 - val_loss: 0.0937 - val_r_square: 0.3003\n",
      "Epoch 55/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1024 - r_square: 0.2377 - val_loss: 0.0922 - val_r_square: 0.3140\n",
      "Epoch 56/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1008 - r_square: 0.2516 - val_loss: 0.0909 - val_r_square: 0.3272\n",
      "Epoch 57/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0994 - r_square: 0.2636 - val_loss: 0.0896 - val_r_square: 0.3391\n",
      "Epoch 58/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0980 - r_square: 0.2754 - val_loss: 0.0884 - val_r_square: 0.3503\n",
      "Epoch 59/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0967 - r_square: 0.2861 - val_loss: 0.0872 - val_r_square: 0.3606\n",
      "Epoch 60/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0954 - r_square: 0.2958 - val_loss: 0.0861 - val_r_square: 0.3698\n",
      "Epoch 61/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0942 - r_square: 0.3044 - val_loss: 0.0850 - val_r_square: 0.3785\n",
      "Epoch 62/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0930 - r_square: 0.3133 - val_loss: 0.0838 - val_r_square: 0.3880\n",
      "Epoch 63/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0918 - r_square: 0.3217 - val_loss: 0.0827 - val_r_square: 0.3960\n",
      "Epoch 64/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0906 - r_square: 0.3296 - val_loss: 0.0816 - val_r_square: 0.4038\n",
      "Epoch 65/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0894 - r_square: 0.3368 - val_loss: 0.0804 - val_r_square: 0.4118\n",
      "Epoch 66/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0883 - r_square: 0.3449 - val_loss: 0.0793 - val_r_square: 0.4195\n",
      "Epoch 67/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0871 - r_square: 0.3514 - val_loss: 0.0782 - val_r_square: 0.4260\n",
      "Epoch 68/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0859 - r_square: 0.3591 - val_loss: 0.0771 - val_r_square: 0.4338\n",
      "Epoch 69/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0847 - r_square: 0.3662 - val_loss: 0.0759 - val_r_square: 0.4413\n",
      "Epoch 70/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0835 - r_square: 0.3731 - val_loss: 0.0748 - val_r_square: 0.4484\n",
      "Epoch 71/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0824 - r_square: 0.3794 - val_loss: 0.0736 - val_r_square: 0.4546\n",
      "Epoch 72/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0812 - r_square: 0.3866 - val_loss: 0.0725 - val_r_square: 0.4626\n",
      "Epoch 73/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0800 - r_square: 0.3934 - val_loss: 0.0713 - val_r_square: 0.4692\n",
      "Epoch 74/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0788 - r_square: 0.3991 - val_loss: 0.0701 - val_r_square: 0.4758\n",
      "Epoch 75/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0776 - r_square: 0.4067 - val_loss: 0.0690 - val_r_square: 0.4833\n",
      "Epoch 76/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0764 - r_square: 0.4134 - val_loss: 0.0678 - val_r_square: 0.4900\n",
      "Epoch 77/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0753 - r_square: 0.4201 - val_loss: 0.0667 - val_r_square: 0.4973\n",
      "Epoch 78/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0741 - r_square: 0.4260 - val_loss: 0.0656 - val_r_square: 0.5040\n",
      "Epoch 79/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0729 - r_square: 0.4332 - val_loss: 0.0645 - val_r_square: 0.5109\n",
      "Epoch 80/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0718 - r_square: 0.4391 - val_loss: 0.0633 - val_r_square: 0.5175\n",
      "Epoch 81/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0706 - r_square: 0.4453 - val_loss: 0.0623 - val_r_square: 0.5248\n",
      "Epoch 82/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0695 - r_square: 0.4527 - val_loss: 0.0612 - val_r_square: 0.5319\n",
      "Epoch 83/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0684 - r_square: 0.4591 - val_loss: 0.0600 - val_r_square: 0.5382\n",
      "Epoch 84/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0673 - r_square: 0.4652 - val_loss: 0.0590 - val_r_square: 0.5448\n",
      "Epoch 85/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0662 - r_square: 0.4726 - val_loss: 0.0580 - val_r_square: 0.5515\n",
      "Epoch 86/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0651 - r_square: 0.4787 - val_loss: 0.0570 - val_r_square: 0.5582\n",
      "Epoch 87/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0641 - r_square: 0.4856 - val_loss: 0.0560 - val_r_square: 0.5655\n",
      "Epoch 88/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0631 - r_square: 0.4915 - val_loss: 0.0550 - val_r_square: 0.5713\n",
      "Epoch 89/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0621 - r_square: 0.4972 - val_loss: 0.0541 - val_r_square: 0.5778\n",
      "Epoch 90/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0611 - r_square: 0.5037 - val_loss: 0.0531 - val_r_square: 0.5835\n",
      "Epoch 91/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0602 - r_square: 0.5100 - val_loss: 0.0523 - val_r_square: 0.5901\n",
      "Epoch 92/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0592 - r_square: 0.5149 - val_loss: 0.0514 - val_r_square: 0.5959\n",
      "Epoch 93/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0583 - r_square: 0.5211 - val_loss: 0.0506 - val_r_square: 0.6025\n",
      "Epoch 94/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0575 - r_square: 0.5276 - val_loss: 0.0498 - val_r_square: 0.6084\n",
      "Epoch 95/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0566 - r_square: 0.5329 - val_loss: 0.0490 - val_r_square: 0.6140\n",
      "Epoch 96/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0558 - r_square: 0.5384 - val_loss: 0.0482 - val_r_square: 0.6191\n",
      "Epoch 97/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0550 - r_square: 0.5446 - val_loss: 0.0476 - val_r_square: 0.6252\n",
      "Epoch 98/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0542 - r_square: 0.5494 - val_loss: 0.0468 - val_r_square: 0.6305\n",
      "Epoch 99/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0535 - r_square: 0.5541 - val_loss: 0.0461 - val_r_square: 0.6352\n",
      "Epoch 100/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0528 - r_square: 0.5598 - val_loss: 0.0455 - val_r_square: 0.6403\n",
      "Epoch 101/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0521 - r_square: 0.5648 - val_loss: 0.0449 - val_r_square: 0.6455\n",
      "Epoch 102/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0514 - r_square: 0.5696 - val_loss: 0.0443 - val_r_square: 0.6499\n",
      "Epoch 103/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0508 - r_square: 0.5737 - val_loss: 0.0437 - val_r_square: 0.6546\n",
      "Epoch 104/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0501 - r_square: 0.5789 - val_loss: 0.0432 - val_r_square: 0.6594\n",
      "Epoch 105/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0495 - r_square: 0.5843 - val_loss: 0.0427 - val_r_square: 0.6637\n",
      "Epoch 106/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0490 - r_square: 0.5886 - val_loss: 0.0422 - val_r_square: 0.6676\n",
      "Epoch 107/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0484 - r_square: 0.5918 - val_loss: 0.0416 - val_r_square: 0.6713\n",
      "Epoch 108/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0479 - r_square: 0.5970 - val_loss: 0.0413 - val_r_square: 0.6758\n",
      "Epoch 109/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0474 - r_square: 0.6004 - val_loss: 0.0409 - val_r_square: 0.6798\n",
      "Epoch 110/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0469 - r_square: 0.6045 - val_loss: 0.0405 - val_r_square: 0.6835\n",
      "Epoch 111/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0464 - r_square: 0.6072 - val_loss: 0.0400 - val_r_square: 0.6865\n",
      "Epoch 112/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0459 - r_square: 0.6122 - val_loss: 0.0397 - val_r_square: 0.6902\n",
      "Epoch 113/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0455 - r_square: 0.6161 - val_loss: 0.0394 - val_r_square: 0.6936\n",
      "Epoch 114/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0451 - r_square: 0.6183 - val_loss: 0.0390 - val_r_square: 0.6967\n",
      "Epoch 115/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0447 - r_square: 0.6222 - val_loss: 0.0387 - val_r_square: 0.7000\n",
      "Epoch 116/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0443 - r_square: 0.6255 - val_loss: 0.0385 - val_r_square: 0.7033\n",
      "Epoch 117/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0439 - r_square: 0.6287 - val_loss: 0.0380 - val_r_square: 0.7057\n",
      "Epoch 118/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0435 - r_square: 0.6311 - val_loss: 0.0378 - val_r_square: 0.7086\n",
      "Epoch 119/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0431 - r_square: 0.6350 - val_loss: 0.0375 - val_r_square: 0.7113\n",
      "Epoch 120/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0427 - r_square: 0.6383 - val_loss: 0.0373 - val_r_square: 0.7143\n",
      "Epoch 121/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0424 - r_square: 0.6399 - val_loss: 0.0369 - val_r_square: 0.7165\n",
      "Epoch 122/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0420 - r_square: 0.6440 - val_loss: 0.0368 - val_r_square: 0.7194\n",
      "Epoch 123/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0417 - r_square: 0.6460 - val_loss: 0.0365 - val_r_square: 0.7217\n",
      "Epoch 124/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0414 - r_square: 0.6488 - val_loss: 0.0362 - val_r_square: 0.7241\n",
      "Epoch 125/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0411 - r_square: 0.6517 - val_loss: 0.0361 - val_r_square: 0.7265\n",
      "Epoch 126/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0407 - r_square: 0.6541 - val_loss: 0.0357 - val_r_square: 0.7287\n",
      "Epoch 127/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0404 - r_square: 0.6557 - val_loss: 0.0355 - val_r_square: 0.7311\n",
      "Epoch 128/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0401 - r_square: 0.6591 - val_loss: 0.0353 - val_r_square: 0.7333\n",
      "Epoch 129/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0398 - r_square: 0.6617 - val_loss: 0.0351 - val_r_square: 0.7355\n",
      "Epoch 130/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0395 - r_square: 0.6632 - val_loss: 0.0349 - val_r_square: 0.7376\n",
      "Epoch 131/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0392 - r_square: 0.6654 - val_loss: 0.0349 - val_r_square: 0.7397\n",
      "Epoch 132/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0389 - r_square: 0.6687 - val_loss: 0.0347 - val_r_square: 0.7417\n",
      "Epoch 133/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0387 - r_square: 0.6704 - val_loss: 0.0344 - val_r_square: 0.7438\n",
      "Epoch 134/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0384 - r_square: 0.6724 - val_loss: 0.0342 - val_r_square: 0.7456\n",
      "Epoch 135/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0381 - r_square: 0.6746 - val_loss: 0.0342 - val_r_square: 0.7476\n",
      "Epoch 136/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0379 - r_square: 0.6758 - val_loss: 0.0341 - val_r_square: 0.7494\n",
      "Epoch 137/140\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0376 - r_square: 0.6794 - val_loss: 0.0340 - val_r_square: 0.7512\n",
      "Epoch 138/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0374 - r_square: 0.6814 - val_loss: 0.0339 - val_r_square: 0.7529\n",
      "Epoch 139/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0371 - r_square: 0.6830 - val_loss: 0.0340 - val_r_square: 0.7544\n",
      "Epoch 140/140\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0369 - r_square: 0.6849 - val_loss: 0.0336 - val_r_square: 0.7563\n",
      "23/23 [==============================] - 0s 816us/step - loss: 0.0336 - r_square: 0.7563\n",
      "Test loss: [0.033565454185009, 0.7563226222991943]\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_logarithmic_error', optimizer=opt, metrics=[RSquare()])\n",
    "model.fit(X_train, y_train,epochs=140,validation_data=(X_test,y_test))\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predicted values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 680us/step\n",
      "Score on test split: 0.7563223666519547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Score on test split:\",r2_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adding layers | overfitting detector / auto_LR_reduction\n",
    "1) Add more layer, trying with 5\n",
    "2) Add overfitting detector that will stop before complete all the epoch if needed\n",
    "3) Add learning rate optimizer that will reduce the LR automatically\n",
    "4) It is a general practice to reduce the amount of neurons to half of the previous layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "69/69 [==============================] - 1s 3ms/step - loss: 23.6571 - r_square: -3.9874 - val_loss: 0.9450 - val_r_square: -1.9425 - lr: 0.0080\n",
      "Epoch 2/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3720 - r_square: -0.7472 - val_loss: 0.1280 - val_r_square: 0.0513 - lr: 0.0080\n",
      "Epoch 3/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1162 - r_square: 0.1601 - val_loss: 0.0985 - val_r_square: 0.3418 - lr: 0.0080\n",
      "Epoch 4/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1022 - r_square: 0.2828 - val_loss: 0.0893 - val_r_square: 0.3794 - lr: 0.0080\n",
      "Epoch 5/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0930 - r_square: 0.3247 - val_loss: 0.0802 - val_r_square: 0.4290 - lr: 0.0080\n",
      "Epoch 6/500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0837 - r_square: 0.3815 - val_loss: 0.0706 - val_r_square: 0.4651 - lr: 0.0080\n",
      "Epoch 7/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0745 - r_square: 0.4198 - val_loss: 0.0628 - val_r_square: 0.5366 - lr: 0.0080\n",
      "Epoch 8/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0655 - r_square: 0.4801 - val_loss: 0.0536 - val_r_square: 0.5824 - lr: 0.0080\n",
      "Epoch 9/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0577 - r_square: 0.5279 - val_loss: 0.0470 - val_r_square: 0.6268 - lr: 0.0080\n",
      "Epoch 10/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0517 - r_square: 0.5707 - val_loss: 0.0423 - val_r_square: 0.6619 - lr: 0.0080\n",
      "Epoch 11/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0476 - r_square: 0.6055 - val_loss: 0.0398 - val_r_square: 0.6917 - lr: 0.0080\n",
      "Epoch 12/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0445 - r_square: 0.6309 - val_loss: 0.0367 - val_r_square: 0.7079 - lr: 0.0080\n",
      "Epoch 13/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0424 - r_square: 0.6459 - val_loss: 0.0360 - val_r_square: 0.7270 - lr: 0.0080\n",
      "Epoch 14/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0405 - r_square: 0.6626 - val_loss: 0.0354 - val_r_square: 0.7397 - lr: 0.0080\n",
      "Epoch 15/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0389 - r_square: 0.6738 - val_loss: 0.0346 - val_r_square: 0.7497 - lr: 0.0080\n",
      "Epoch 16/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0375 - r_square: 0.6852 - val_loss: 0.0356 - val_r_square: 0.7561 - lr: 0.0080\n",
      "Epoch 17/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0366 - r_square: 0.6960 - val_loss: 0.0335 - val_r_square: 0.7675 - lr: 0.0080\n",
      "Epoch 18/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0354 - r_square: 0.7046 - val_loss: 0.0310 - val_r_square: 0.7786 - lr: 0.0080\n",
      "Epoch 19/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0343 - r_square: 0.7133 - val_loss: 0.0312 - val_r_square: 0.7853 - lr: 0.0080\n",
      "Epoch 20/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0334 - r_square: 0.7212 - val_loss: 0.0351 - val_r_square: 0.7794 - lr: 0.0080\n",
      "Epoch 21/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0325 - r_square: 0.7294 - val_loss: 0.0295 - val_r_square: 0.8009 - lr: 0.0080\n",
      "Epoch 22/500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0316 - r_square: 0.7370 - val_loss: 0.0287 - val_r_square: 0.8085 - lr: 0.0080\n",
      "Epoch 23/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0309 - r_square: 0.7437 - val_loss: 0.0320 - val_r_square: 0.8035 - lr: 0.0080\n",
      "Epoch 24/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0302 - r_square: 0.7511 - val_loss: 0.0295 - val_r_square: 0.8166 - lr: 0.0080\n",
      "Epoch 25/500\n",
      "52/69 [=====================>........] - ETA: 0s - loss: 0.0300 - r_square: 0.7444\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.003200000151991844.\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0293 - r_square: 0.7603 - val_loss: 0.0321 - val_r_square: 0.8100 - lr: 0.0080\n",
      "Epoch 25: early stopping\n",
      "23/23 [==============================] - 0s 817us/step - loss: 0.0321 - r_square: 0.8100\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "overfitting_detector = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                        min_delta=0,# minimum change to qualify an improvement\n",
    "                                                        patience=3, #number of epoch after training will be stopped if we have no improvement\n",
    "                                                        verbose=1,\n",
    "                                                        mode='auto', # let algorithm decide when to stop based on quantity monitored\n",
    "                                                        baseline=None, # we decide to not use any threshold for stopping\n",
    "                                                        restore_best_weights=False) #restore last model from the best quality epoch previosly found\n",
    "\n",
    "adjsutable_learning_rate =tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                                               factor=0.4, # factor that decide the reduction of LR\n",
    "                                                               patience=3, #number of epoch after LR will be reduced\n",
    "                                                               verbose=1,\n",
    "                                                               mode=\"auto\",\n",
    "                                                               min_delta=0.0001, # generic used parameter with this particular value (default)\n",
    "                                                               cooldown=0, # make reduce LR always active\n",
    "                                                               min_lr=0) # decide if specify a lower bound to the LR\n",
    "\n",
    "model.add(layers.Dense(units=189, kernel_initializer='normal', activation='relu', input_shape=[188]))\n",
    "model.add(layers.Dense(units=95, kernel_initializer='normal', activation='relu'))\n",
    "model.add(layers.Dense(units=47, kernel_initializer='normal', activation='relu'))\n",
    "model.add(layers.Dense(units=28, kernel_initializer='normal', activation='relu'))\n",
    "model.add(layers.Dense(units=1, kernel_initializer='normal', activation=None))\n",
    "\n",
    "# Increment learning rate because we have a way to stop before the end of all epochs = overfitting detector\n",
    "# Also the learning rate will be automatically reduced by auto_LR_reduction\n",
    "opt = optimizers.Adam(learning_rate=0.008,beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_logarithmic_error', optimizer=opt, metrics=[RSquare()])\n",
    "model.fit(X_train, y_train,epochs=500,\n",
    "          callbacks=[overfitting_detector,adjsutable_learning_rate],\n",
    "          workers=50, # number of thread parallel if cpu have all used, it will start use GPU\n",
    "          validation_data=(X_test,y_test)) #split used for test the model\n",
    "\n",
    "loss = model.evaluate(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note how the new model is improved in velocity and score thanks to auto_LR_reduction and overfitting detector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adding more layer with common half neurons technique"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "69/69 [==============================] - 1s 4ms/step - loss: 43.3858 - r_square: -3.9817 - val_loss: 0.4407 - val_r_square: -1.0326 - lr: 0.0080\n",
      "Epoch 2/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1784 - r_square: -0.1295 - val_loss: 0.1034 - val_r_square: 0.2753 - lr: 0.0080\n",
      "Epoch 3/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1077 - r_square: 0.2514 - val_loss: 0.0945 - val_r_square: 0.3443 - lr: 0.0080\n",
      "Epoch 4/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0979 - r_square: 0.3003 - val_loss: 0.0844 - val_r_square: 0.4065 - lr: 0.0080\n",
      "Epoch 5/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0868 - r_square: 0.3665 - val_loss: 0.0731 - val_r_square: 0.4733 - lr: 0.0080\n",
      "Epoch 6/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0758 - r_square: 0.4319 - val_loss: 0.0627 - val_r_square: 0.5310 - lr: 0.0080\n",
      "Epoch 7/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0651 - r_square: 0.4893 - val_loss: 0.0495 - val_r_square: 0.6020 - lr: 0.0080\n",
      "Epoch 8/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0513 - r_square: 0.5693 - val_loss: 0.0387 - val_r_square: 0.6894 - lr: 0.0080\n",
      "Epoch 9/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0430 - r_square: 0.6369 - val_loss: 0.0367 - val_r_square: 0.7260 - lr: 0.0080\n",
      "Epoch 10/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0391 - r_square: 0.6705 - val_loss: 0.0340 - val_r_square: 0.7474 - lr: 0.0080\n",
      "Epoch 11/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0363 - r_square: 0.6903 - val_loss: 0.0363 - val_r_square: 0.7535 - lr: 0.0080\n",
      "Epoch 12/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0347 - r_square: 0.7049 - val_loss: 0.0321 - val_r_square: 0.7739 - lr: 0.0080\n",
      "Epoch 13/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0331 - r_square: 0.7168 - val_loss: 0.0320 - val_r_square: 0.7822 - lr: 0.0080\n",
      "Epoch 14/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0319 - r_square: 0.7279 - val_loss: 0.0371 - val_r_square: 0.7704 - lr: 0.0080\n",
      "Epoch 15/500\n",
      "44/69 [==================>...........] - ETA: 0s - loss: 0.0315 - r_square: 0.7422\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.003200000151991844.\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0313 - r_square: 0.7350 - val_loss: 0.0336 - val_r_square: 0.7879 - lr: 0.0080\n",
      "Epoch 16/500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0302 - r_square: 0.7408 - val_loss: 0.0328 - val_r_square: 0.7921 - lr: 0.0032\n",
      "Epoch 16: early stopping\n",
      "23/23 [==============================] - 0s 852us/step - loss: 0.0328 - r_square: 0.7921\n",
      "Test loss: [0.03284100443124771, 0.7921148538589478]\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "overfitting_detector = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                        min_delta=0,# minimum change to qualify an improvement\n",
    "                                                        patience=3, #number of epoch after training will be stopped if we have no improvement\n",
    "                                                        verbose=1,\n",
    "                                                        mode='auto', # let algorithm decide when to stop based on quantity monitored\n",
    "                                                        baseline=None, # we decide to not use any threshold for stopping\n",
    "                                                        restore_best_weights=False) #restore last model from the best quality epoch previosly found\n",
    "\n",
    "adjsutable_learning_rate =tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                                               factor=0.4, # factor that decide the reduction of LR\n",
    "                                                               patience=3, #number of epoch after LR will be reduced\n",
    "                                                               verbose=1,\n",
    "                                                               mode=\"auto\",\n",
    "                                                               min_delta=0.0001, # generic used parameter with this particular value (default)\n",
    "                                                               cooldown=0, # make reduce LR always active\n",
    "                                                               min_lr=0) # decide if specify a lower bound to the LR\n",
    "\n",
    "model.add(layers.Dense(units=189, kernel_initializer='normal', activation='relu', input_shape=[188]))\n",
    "model.add(layers.Dense(units=95, kernel_initializer='normal', activation='relu'))\n",
    "model.add(layers.Dense(units=47, kernel_initializer='normal', activation='relu'))\n",
    "model.add(layers.Dense(units=28, kernel_initializer='normal', activation='relu'))\n",
    "model.add(layers.Dense(units=14, kernel_initializer='normal', activation='relu'))\n",
    "model.add(layers.Dense(units=7, kernel_initializer='normal', activation='relu'))\n",
    "model.add(layers.Dense(units=3, kernel_initializer='normal', activation='relu'))\n",
    "model.add(layers.Dense(units=1, kernel_initializer='normal', activation=None))\n",
    "\n",
    "# Increment learning rate because we have a way to stop before the end of all epochs = overfitting detector\n",
    "# Also the learning rate will be automatically reduced by auto_LR_reduction\n",
    "opt = optimizers.Adam(learning_rate=0.008,beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_logarithmic_error', optimizer=opt, metrics=[RSquare()])\n",
    "model.fit(X_train, y_train,epochs=500,\n",
    "          callbacks=[overfitting_detector,adjsutable_learning_rate],\n",
    "          workers=50, # number of thread parallel if cpu have all used, it will start use GPU\n",
    "          validation_data=(X_test,y_test)) #split used for test the model\n",
    "\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "No visible improvement both on the train and test score based on R2."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
